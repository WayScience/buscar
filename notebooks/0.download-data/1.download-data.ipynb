{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14772b4",
   "metadata": {},
   "source": [
    "# Downloading Single-Cell Profiles\n",
    "\n",
    "This notebook focuses on downloading metadata and single-cell profiles from three key datasets:\n",
    "\n",
    "1. **CPJUMP1 Pilot Dataset** ([link](https://github.com/jump-cellpainting/2024_Chandrasekaran_NatureMethods_CPJUMP1)): Metadata is downloaded and processed to identify and organize plates containing wells treated with CRISPR perturbations for downstream analysis.\n",
    "2. **MitoCheck Dataset**: Normalized and feature-selected single-cell profiles are downloaded for further analysis.\n",
    "3. **CFReT Dataset**: Normalized and feature-selected single-cell profiles from the CFReT plate are downloaded for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7748e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "import pathlib\n",
    "\n",
    "import requests\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils import io_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7911fff",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39105354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "def download_compressed_file(\n",
    "    source_url: str, output_path: pathlib.Path | str, chunk_size: int = 8192, extract: bool = True\n",
    "):\n",
    "    \"\"\" Downloads a compressed file from a URL with progress tracking.\n",
    "\n",
    "    Downloads a file from the specified URL and saves it to the given output path.\n",
    "    The download is performed in chunks to handle large files efficiently, and the progress is displayed using\n",
    "    the `tqdm` library. The function raises exceptions for various error conditions, including\n",
    "    invalid input types, file system errors, and issues during the download process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_url : str\n",
    "        URL to download the file from.\n",
    "    output_path : pathlib.Path\n",
    "        Full path where the file should be saved.\n",
    "    chunk_size : int, optional\n",
    "        Size of chunks to download in bytes. Defaults to 8192.\n",
    "    extract : bool, optional\n",
    "        Whether to extract the compressed file after download. Defaults to True.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    requests.exceptions.RequestException\n",
    "        If there is an error during the download request.\n",
    "    Exception\n",
    "        For any unexpected error during file writing or progress tracking.\n",
    "    \"\"\"\n",
    "\n",
    "    # type checking\n",
    "    if not isinstance(source_url, str):\n",
    "        raise TypeError(f\"source_url must be a string, got {type(source_url)}\")\n",
    "    if not isinstance(output_path, (pathlib.Path, str)):\n",
    "        raise TypeError(\n",
    "            f\"output_path must be a pathlib.Path or str, got {type(output_path)}\"\n",
    "        )\n",
    "    if isinstance(output_path, str):\n",
    "        output_path = pathlib.Path(output_path)\n",
    "    if not output_path.parent.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Output directory {output_path.parent} does not exist.\"\n",
    "        )\n",
    "    if output_path.exists() and not output_path.is_file():\n",
    "        raise FileExistsError(f\"Output path {output_path} exists and is not a file.\")\n",
    "\n",
    "    # starting downloading process\n",
    "    try:\n",
    "        # sending GET request to the source URL\n",
    "        with requests.get(source_url, stream=True) as response:\n",
    "            # raise an error if the request was unsuccessful\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # get the total size of the file from the response headers\n",
    "            total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "            # using tqdm to track the download progress\n",
    "            with (\n",
    "                open(output_path, \"wb\") as file,\n",
    "                tqdm(\n",
    "                    desc=\"Downloading\",\n",
    "                    total=total_size,\n",
    "                    unit=\"B\",\n",
    "                    unit_scale=True,\n",
    "                    unit_divisor=1024,\n",
    "                ) as pbar,\n",
    "            ):\n",
    "                # iterating over the response content in chunks\n",
    "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        \n",
    "                        # this updates the progress bar\n",
    "                        pbar.update(len(chunk))\n",
    "\n",
    "        # extract the file if requested\n",
    "        if extract:\n",
    "\n",
    "            # ensring that the path is a directory if the output path is a file\n",
    "            # this is necessary for extraction\n",
    "            extract_dir = output_path\n",
    "            if extract_dir.is_file():\n",
    "                extract_dir = output_path.parent\n",
    "            \n",
    "            if output_path.suffix == '.gz':\n",
    "                # handle gzip files\n",
    "                extracted_path = output_path.with_suffix('')\n",
    "                with gzip.open(output_path, 'rb') as f_in:\n",
    "                    with open(extracted_path, 'wb') as f_out:\n",
    "                        f_out.write(f_in.read())\n",
    "                print(f\"Extracted to: {extracted_path}\")\n",
    "                \n",
    "            elif output_path.suffix == '.zip':\n",
    "                # handle zip files\n",
    "                with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extract_dir)\n",
    "                print(f\"Extracted to: {extract_dir}\")\n",
    "                \n",
    "            elif output_path.suffix in ['.tar', '.tgz'] or '.tar.' in output_path.name:\n",
    "                # handle tar files\n",
    "                with tarfile.open(output_path, 'r:*') as tar_ref:\n",
    "                    tar_ref.extractall(extract_dir)\n",
    "                print(f\"Extracted to: {extract_dir}\")\n",
    "\n",
    "    # handling exceptions\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise requests.exceptions.RequestException(f\"Error downloading file: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2647c06",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7afb5",
   "metadata": {},
   "source": [
    "Parameters used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting perturbation type\n",
    "# other options are \"compound\", \"orf\",\n",
    "pert_type = \"crispr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff431a",
   "metadata": {},
   "source": [
    "setting input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7381913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting config path\n",
    "config_path = pathlib.Path(\"../nb-configs.yaml\").resolve(strict=True)\n",
    "\n",
    "# setting results setting a data directory\n",
    "data_dir = pathlib.Path(\"./data\").resolve()\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# setting a path to save the experimental metadata\n",
    "exp_metadata_path = (data_dir / \"CPJUMP1-experimental-metadata.csv\").resolve()\n",
    "\n",
    "# setting profile directory\n",
    "profiles_dir = (data_dir / \"sc-profiles\").resolve()\n",
    "profiles_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# create mitocheck directory\n",
    "mitocheck_dir = (profiles_dir / \"mitocheck\").resolve()\n",
    "mitocheck_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# create cfret directory\n",
    "cfret_dir = (profiles_dir / \"cfret\").resolve()\n",
    "cfret_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d0b8e",
   "metadata": {},
   "source": [
    "## Downloading CPJUMP1 Metadata\n",
    "\n",
    "In this section, we download and process the CPJUMP1 experimental metadata. This metadata contains information about assay plates, batches, and perturbation types, which is essential for organizing and analyzing single-cell profiles. Only plates treated with CRISPR perturbations are selected for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8bfe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading config file and setting experimental metadata URL\n",
    "nb_configs = io_utils.load_configs(config_path)\n",
    "CPJUMP1_exp_metadata_url = nb_configs[\"links\"][\"CPJUMP1-experimental-metadata-source\"]\n",
    "\n",
    "# read in the experimental metadata CSV file and only filter down to plays that\n",
    "# have an CRISPR perturbation\n",
    "exp_metadata = pl.read_csv(\n",
    "    CPJUMP1_exp_metadata_url, separator=\"\\t\", has_header=True, encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# filtering the metadata to only includes plates that their perturbation types are crispr\n",
    "exp_metadata = exp_metadata.filter(exp_metadata[\"Perturbation\"].str.contains(pert_type))\n",
    "\n",
    "# save the experimental metadata as a csv file\n",
    "exp_metadata.write_csv(exp_metadata_path)\n",
    "\n",
    "# display\n",
    "exp_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121e37c",
   "metadata": {},
   "source": [
    "Creating a dictionary to group plates by their corresponding experimental batch\n",
    "\n",
    "This step organizes the plate barcodes from the experimental metadata into groups based on their batch. Grouping plates by batch is useful for batch-wise data processing and downstream analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c20b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary for the batch and the associated plates with the a batch\n",
    "batch_plates_dict = {}\n",
    "exp_metadata_batches = exp_metadata[\"Batch\"].unique().to_list()\n",
    "\n",
    "for batch in exp_metadata_batches:\n",
    "    batch_plates_dict[batch] = exp_metadata.filter(exp_metadata[\"Batch\"] == batch)[\n",
    "        \"Assay_Plate_Barcode\"\n",
    "    ].to_list()\n",
    "\n",
    "# display batch (Keys) and plates (values) within each batch\n",
    "pprint.pprint(batch_plates_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021b414",
   "metadata": {},
   "source": [
    "## Downloading MitoCheck Data\n",
    "\n",
    "In this section, we download the MitoCheck data generated in [this study](https://pmc.ncbi.nlm.nih.gov/articles/PMC3108885/).\n",
    "\n",
    "Specifically, we are downloading data that has already been normalized and feature-selected. The normalization and feature selection pipeline is available [here](https://github.com/WayScience/mitocheck_data/tree/main/3.normalize_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06783224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url source for the MitoCheck data\n",
    "mitocheck_url = nb_configs[\"links\"][\"MitoCheck-profiles-source\"]\n",
    "output_path = mitocheck_dir / \"mitocheck_profile.zip\"\n",
    "\n",
    "# checking if the downloaded file already exists\n",
    "if output_path.exists():\n",
    "    print(f\"File {output_path} already exists. Skipping download.\")\n",
    "else:\n",
    "    # downloading mitocheck profiles\n",
    "    download_compressed_file(\n",
    "        source_url=mitocheck_url,\n",
    "        output_path=output_path,\n",
    "        chunk_size=8192,\n",
    "        extract=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e09e5",
   "metadata": {},
   "source": [
    "## Downloading CFReT Data\n",
    "\n",
    "In this section, we download feature-selected single-cell profiles from the CFReT plate `localhost230405150001`. This plate contains three treatments: DMSO (control), drug_x, and TGFRi. The dataset consists of high-content imaging data that has already undergone feature selection, making it suitable for downstream analysis.\n",
    "\n",
    "**Key Points:**\n",
    "- Only the processed single-cell profiles are downloaded [here](https://github.com/WayScience/cellpainting_predicts_cardiac_fibrosis/tree/main/3.process_cfret_features/data/single_cell_profiles)\n",
    "- The CFReT dataset was used and published in [this study](https://doi.org/10.1161/CIRCULATIONAHA.124.071956)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the source for the CFReT data\n",
    "cfret_source = nb_configs[\"links\"][\"CFReT-profiles-source\"]\n",
    "\n",
    "# use the correct filename from the source URL\n",
    "output_path = (cfret_dir / \"localhost230405150001_sc_feature_selected.parquet\").resolve()\n",
    "\n",
    "# checking if the download already exists if it does not exist \n",
    "# download the file\n",
    "if output_path.exists():\n",
    "    print(f\"File {output_path} already exists. Skipping download.\")\n",
    "else:\n",
    "    download_compressed_file(\n",
    "        source_url=cfret_source, \n",
    "        output_path=output_path, \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buscar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
