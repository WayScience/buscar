{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e412a0f9",
   "metadata": {},
   "source": [
    "# 2. Preprocessing Data\n",
    "\n",
    "This notebook demonstrates how to preprocess single-cell profile data for downstream analysis. It covers the following steps:\n",
    "\n",
    "**Overview**\n",
    "\n",
    "- **Data Exploration**: Examining the structure and contents of the downloaded datasets\n",
    "- **Metadata Handling**: Loading experimental metadata to guide data selection and organization\n",
    "- **Feature Selection**: Applying a shared feature space for consistency across datasets\n",
    "- **Profile Concatenation**: Merging profiles from multiple experimental plates into a unified DataFrame\n",
    "- **Format Conversion**: Converting raw CSV files to Parquet format for efficient storage and access\n",
    "- **Metadata and Feature Documentation**: Saving metadata and feature information to ensure reproducibility\n",
    "\n",
    "These preprocessing steps ensure that all datasets are standardized, well-documented, and ready for comparative and integrative analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0387feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Optional\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils.data_utils import split_meta_and_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33f13a",
   "metadata": {},
   "source": [
    "## Helper functions \n",
    "\n",
    "Contains helper function that pertains to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f8b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_profiles(\n",
    "    profile_dir: str | pathlib.Path,\n",
    "    shared_features: Optional[list[str]] = None,\n",
    "    specific_plates: Optional[list[pathlib.Path]] = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all profile files from a directory and concatenate them into a single Polars DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profile_dir : str or pathlib.Path\n",
    "        Directory containing the profile files (.parquet).\n",
    "    shared_features : Optional[list[str]], optional\n",
    "        List of shared feature names to filter the profiles. If None, all features are loaded.\n",
    "    specific_plates : Optional[list[pathlib.Path]], optional\n",
    "        List of specific plate file paths to load. If None, all profiles in the directory are loaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Concatenated Polars DataFrame containing all loaded profiles.\n",
    "    \"\"\"\n",
    "    # Ensure profile_dir is a pathlib.Path\n",
    "    if isinstance(profile_dir, str):\n",
    "        profile_dir = pathlib.Path(profile_dir)\n",
    "    elif not isinstance(profile_dir, pathlib.Path):\n",
    "        raise TypeError(\"profile_dir must be a string or a pathlib.Path object\")\n",
    "\n",
    "    # Validate specific_plates\n",
    "    if specific_plates is not None:\n",
    "        if not isinstance(specific_plates, list):\n",
    "            raise TypeError(\"specific_plates must be a list of pathlib.Path objects\")\n",
    "        if not all(isinstance(path, pathlib.Path) for path in specific_plates):\n",
    "            raise TypeError(\n",
    "                \"All elements in specific_plates must be pathlib.Path objects\"\n",
    "            )\n",
    "\n",
    "    def load_profile(file: pathlib.Path) -> pl.DataFrame:\n",
    "        \"\"\"internal function to load a single profile file.\n",
    "        \"\"\"\n",
    "        profile_df = pl.read_parquet(file)\n",
    "        meta_cols, _ = split_meta_and_features(profile_df)\n",
    "        if shared_features is not None:\n",
    "            # Only select metadata and shared features\n",
    "            return profile_df.select(meta_cols + shared_features)\n",
    "        return profile_df\n",
    "\n",
    "    # Use specific_plates if provided, otherwise gather all .parquet files\n",
    "    if specific_plates is not None:\n",
    "        # Validate that all specific plate files exist\n",
    "        for plate_path in specific_plates:\n",
    "            if not plate_path.exists():\n",
    "                raise FileNotFoundError(f\"Profile file not found: {plate_path}\")\n",
    "        files_to_load = specific_plates\n",
    "    else:\n",
    "        files_to_load = list(profile_dir.glob(\"*.parquet\"))\n",
    "        if not files_to_load:\n",
    "            raise FileNotFoundError(f\"No profile files found in {profile_dir}\")\n",
    "\n",
    "    # Load and concatenate profiles\n",
    "    loaded_profiles = [load_profile(f) for f in files_to_load]\n",
    "\n",
    "    # Concatenate all loaded profiles\n",
    "    return pl.concat(loaded_profiles, rechunk=True)\n",
    "\n",
    "    \n",
    "def split_data(pycytominer_output: pl.DataFrame, dataset: str = \"CP_and_DP\") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Split pycytominer output to metadata dataframe and feature values using Polars.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pycytominer_output : pl.DataFrame\n",
    "        Polars DataFrame with pycytominer output\n",
    "    dataset : str, optional\n",
    "        Which dataset features to split,\n",
    "        can be \"CP\" or \"DP\" or by default \"CP_and_DP\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Polars DataFrame with metadata and selected features\n",
    "    \"\"\"\n",
    "    all_cols = pycytominer_output.columns\n",
    "\n",
    "    # Get DP, CP, or both features from all columns depending on desired dataset\n",
    "    if dataset == \"CP\":\n",
    "        feature_cols = [col for col in all_cols if \"CP__\" in col]\n",
    "    elif dataset == \"DP\":\n",
    "        feature_cols = [col for col in all_cols if \"DP__\" in col]\n",
    "    elif dataset == \"CP_and_DP\":\n",
    "        feature_cols = [col for col in all_cols if \"P__\" in col]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid dataset '{dataset}'. Choose from 'CP', 'DP', or 'CP_and_DP'.\"\n",
    "        )\n",
    "\n",
    "    # Metadata columns is all columns except feature columns\n",
    "    metadata_cols = [col for col in all_cols if \"P__\" not in col]\n",
    "\n",
    "    # Select metadata and feature columns\n",
    "    selected_cols = metadata_cols + feature_cols\n",
    "    \n",
    "    return pycytominer_output.select(selected_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8d2d9",
   "metadata": {},
   "source": [
    "Defining the input and output directories used throughout the notebook.\n",
    "\n",
    "> **Note:** The shared profiles utilized here are sourced from the [JUMP-single-cell](https://github.com/WayScience/JUMP-single-cell) repository. All preprocessing and profile generation steps are performed in that repository, and this notebook focuses on downstream analysis using the generated profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea207e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting data directory\n",
    "data_dir = pathlib.Path(\"./data\").resolve(strict=True)\n",
    "\n",
    "# Setting profiles directory\n",
    "profiles_dir = (data_dir / \"sc-profiles\").resolve(strict=True)\n",
    "\n",
    "# Experimental metadata\n",
    "exp_metadata_path = (data_dir / \"CPJUMP1-experimental-metadata.csv\").resolve(strict=True)\n",
    "\n",
    "# Setting feature selection path\n",
    "shared_features_config_path = (data_dir / \"feature_selected_sc_qc_features.json\").resolve(strict=True)\n",
    "\n",
    "# setting mitocheck profiles directory\n",
    "mitocheck_profiles_dir = (profiles_dir / \"mitocheck\").resolve(strict=True)\n",
    "mitocheck_norm_profiles_dir = (mitocheck_profiles_dir / \"normalized_data\").resolve(strict=True)\n",
    "\n",
    "# Make a results folder\n",
    "results_dir = pathlib.Path(\"./results\").resolve()\n",
    "results_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168a71a",
   "metadata": {},
   "source": [
    "Create a list of paths that only points crispr treated plates and load the shared features config file that can be found in this [repo](https://github.com/WayScience/JUMP-single-cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7944fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experimental metadata\n",
    "exp_metadata = pl.read_csv(exp_metadata_path)\n",
    "crispr_plate_names = exp_metadata.select(\"Assay_Plate_Barcode\").unique().to_series().to_list()\n",
    "crispr_plate_paths = [\n",
    "        (profiles_dir / f\"{plate}_feature_selected_sc_qc.parquet\") for plate in crispr_plate_names\n",
    "    ]\n",
    "# Load shared features\n",
    "with open(shared_features_config_path) as f:\n",
    "    loaded_shared_features = json.load(f)\n",
    "\n",
    "shared_features = loaded_shared_features[\"shared-features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bfd5c7",
   "metadata": {},
   "source": [
    "## Preprocessing CPJUMP1 CRISPR data\n",
    "\n",
    "Using the filtered CRISPR plate file paths and shared features configuration, we load all individual profile files and concatenate them into a single comprehensive DataFrame. This step combines data from multiple experimental plates while maintaining the consistent feature space defined by the shared features list.\n",
    "\n",
    "The concatenation process ensures:\n",
    "- All profiles use the same feature set for downstream compatibility\n",
    "- Metadata columns are preserved across all plates\n",
    "- Data integrity is maintained during the merge operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f7e08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat profiles already exists, loading from file\n"
     ]
    }
   ],
   "source": [
    "# Loading crispr profiles with shared features and concat into a single DataFrame\n",
    "concat_output_path = results_dir / \"concat_crispr_profiles.parquet\"\n",
    "if concat_output_path.exists():\n",
    "    print(\"concat profiles already exists, loading from file\")\n",
    "else:\n",
    "    loaded_profiles = load_and_concat_profiles(\n",
    "        profile_dir=profiles_dir,\n",
    "        specific_plates=crispr_plate_paths,\n",
    "        shared_features=shared_features\n",
    "    )\n",
    "\n",
    "    # Add index column \n",
    "    loaded_profiles = loaded_profiles.with_row_index(\"index\")\n",
    "\n",
    "    # Split meta and features\n",
    "    meta_cols, features_cols = split_meta_and_features(loaded_profiles)\n",
    "\n",
    "    # Saving metadata and features of the concat profile into a json file\n",
    "    meta_features_dict = {\n",
    "        \"concat-profiles\": {\n",
    "            \"meta-features\": meta_cols,\n",
    "            \"shared-features\": features_cols\n",
    "        }\n",
    "    }\n",
    "    with open(results_dir / \"concat_profiles_meta_features.json\", \"w\") as f:\n",
    "        json.dump(meta_features_dict, f, indent=4)\n",
    "\n",
    "    # Save the concated profiles\n",
    "    loaded_profiles.write_parquet(concat_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ba6ad",
   "metadata": {},
   "source": [
    "## Preprocessing MitoCheck\n",
    "\n",
    "This section processes the MitoCheck dataset by loading the training and negative control data from compressed CSV files. The original CSV format is converted to Parquet for consistency with other processed data and improved performance.\n",
    "\n",
    "Key preprocessing steps include:\n",
    "- **Loading training data**: Reading the main MitoCheck profiles containing various phenotypic classes\n",
    "- **Processing negative controls**: Loading control samples and adding phenotypic class labels\n",
    "- **Feature filtering**: Extracting only Cell Profiler (CP) features to match the CPJUMP1 dataset structure\n",
    "- **Standardization**: Ensuring consistent column naming and metadata structure across datasets\n",
    "- **Feature alignment**: Identifying shared features between training and control data for unified analysis\n",
    "\n",
    "The processed data is saved in Parquet format with optimized storage and maintained metadata integrity, enabling efficient downstream comparative analysis between MitoCheck and CPJUMP1 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5471d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mitocheck profiles and save as parquet\n",
    "mitocheck_profile = pl.read_csv(\n",
    "    mitocheck_norm_profiles_dir / \"training_data.csv.gz\",\n",
    "    )\n",
    "\n",
    "# drop first column by index\n",
    "# This is done to remove the index column that is not needed\n",
    "mitocheck_profile = mitocheck_profile.select(mitocheck_profile.columns[1:])\n",
    "\n",
    "# loading in negative control profiles\n",
    "mitocheck_neg_control_profiles = pl.read_csv(\n",
    "    mitocheck_norm_profiles_dir / \"negative_control_data.csv.gz\",\n",
    "    )\n",
    "\n",
    "# insert new column \"Mitocheck_Phenotypic_Class\"\n",
    "mitocheck_neg_control_profiles = mitocheck_neg_control_profiles.with_columns(\n",
    "    pl.lit(\"negcon\").alias(\"Mitocheck_Phenotypic_Class\")\n",
    ").select([\"Mitocheck_Phenotypic_Class\"] + mitocheck_neg_control_profiles.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c141af",
   "metadata": {},
   "source": [
    "Filter Cell Profiler (CP) features and preprocess columns by removing the \"CP__\" prefix to standardize feature names for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c420c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split profiles \n",
    "cp_mitocheck_profile = split_data(mitocheck_profile, dataset=\"CP\")\n",
    "cp_mitocheck_neg_control_profiles = split_data(mitocheck_neg_control_profiles, dataset=\"CP\")\n",
    "\n",
    "# rename columns to remove \"CP__\" prefix and replace it with \"NoCompartment_\"\n",
    "cp_mitocheck_profile = cp_mitocheck_profile.rename(\n",
    "    lambda x: x.replace(\"CP__\", \"\") if \"CP__\" in x else x\n",
    ")\n",
    "cp_mitocheck_neg_control_profiles = cp_mitocheck_neg_control_profiles.rename(\n",
    "    lambda x: x.replace(\"CP__\", \"\") if \"CP__\" in x else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba5e19",
   "metadata": {},
   "source": [
    "Splitting the metadata and feature columns for each dataset to enable targeted downstream analysis and ensure consistent data structure across all profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a44859fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata features\n",
    "cp_mitocheck_profile_meta = cp_mitocheck_profile.columns[:13]\n",
    "cp_mitocheck_neg_control_profiles_meta = cp_mitocheck_neg_control_profiles.columns[:12]\n",
    "\n",
    "# morphology features \n",
    "cp_mitocheck_profile_features = cp_mitocheck_profile.drop(cp_mitocheck_profile_meta).columns\n",
    "cp_mitocheck_neg_control_profiles_features = cp_mitocheck_neg_control_profiles.drop(cp_mitocheck_neg_control_profiles_meta).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576d0fa",
   "metadata": {},
   "source": [
    "Identify the shared metadata and feature columns between the two datasets, concatenate them into a unified DataFrame containing only these shared columns, and save the result as a Parquet file for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4da5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find shared metadata columns\n",
    "cp_mitocheck_profile_meta_cols = set(cp_mitocheck_profile_meta)\n",
    "cp_mitocheck_neg_control_profiles_meta_cols = set(cp_mitocheck_neg_control_profiles_meta)\n",
    "shared_meta = cp_mitocheck_profile_meta_cols.intersection(cp_mitocheck_neg_control_profiles_meta_cols)\n",
    "\n",
    "# find shared feature columns\n",
    "cp_mitocheck_profile_features_cols = set(cp_mitocheck_profile_features)\n",
    "cp_mitocheck_neg_control_profiles_features_cols = set(cp_mitocheck_neg_control_profiles_features)\n",
    "shared_features = cp_mitocheck_profile_features_cols.intersection(cp_mitocheck_neg_control_profiles_features_cols)\n",
    "    \n",
    "# concat both shared metadata and features\n",
    "final_shared_features = list(shared_meta) + list(shared_features)\n",
    "\n",
    "# select only shared metadata and features from both profiles and concat\n",
    "cp_mitocheck_profiles_path = mitocheck_profiles_dir / \"concat_mitocheck_cp_profiles_shared_feats.parquet\"\n",
    "pl.concat([\n",
    "    cp_mitocheck_profile.select(final_shared_features),\n",
    "    cp_mitocheck_neg_control_profiles.select(final_shared_features)\n",
    "], rechunk=True\n",
    ").write_parquet(cp_mitocheck_profiles_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buscar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
